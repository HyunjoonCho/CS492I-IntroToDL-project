{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdfc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import FinanceDataReader as fdr\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fb2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f674e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43cc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9095e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c25482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360 240\n"
     ]
    }
   ],
   "source": [
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "root = \"newsData/\"\n",
    "list = os.listdir(root)\n",
    "for cat in list:\n",
    "    files = os.listdir(root + cat)\n",
    "    for i,f in enumerate(files):\n",
    "        fname = root + cat + \"/\" + f\n",
    "        file = open(fname, \"r\", encoding=\"utf-8\")\n",
    "        strings = file.read()\n",
    "        if i<170:\n",
    "            dataset_train.append([strings, cat])\n",
    "        else:\n",
    "            dataset_test.append([strings,cat])\n",
    "        file.close()\n",
    "\n",
    "print(len(dataset_train), len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4dbc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d76b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c88401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [len(i[0]) for i in dataset_train]\n",
    "l2 = [len(i[0]) for i in dataset_test]\n",
    "max(max(l1),max(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46c757c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e913c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbbf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b4d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=8,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)\n",
    "\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39a9b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01cf8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5464a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e16d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "689ef5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82809c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5af110e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf431944331e4e97b92c9fcbd6cc7acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.13920454545454544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0d1200f2984153a00c6ba5f98e00d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.4375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4029e3ce16e4b0388c9077f4296d448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.5980113636363636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8801c54e844d5dbc873b8085cc5c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.7682291666666666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d552be9b49433fadbefdb24ef5b57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.8515625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f202caafd6f34da2847c813dbba110a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.8385416666666666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5df8afcdd04b60a89cf58c030ded6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.9183238636363636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9862a1c95a4eada1e26a8ecf0fb4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.8502604166666666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93278e4f5bac45aa802967dc9f63d284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train acc 0.9360795454545454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f640e3682b9471094ac72a5330618df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.8333333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff34109245d84df3a9bc458703ade2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 train acc 0.9637784090909091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5b49595c214caeb06d52990d27feb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.8880208333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1c7545b6f94dfa850ce622b5cf9f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 train acc 0.9715909090909091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3926ac8e619b481eb4c53f0a2cbb6be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.8802083333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62413dff97074d09b12c5ba50199cdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 train acc 0.9886363636363636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f065f71d3e234e838e62ae49420d76ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.8958333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a725404688b14f93bbd20109c5378a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 train acc 0.9900568181818182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b586c36dae2946bf99d363f19a226161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 test acc 0.8958333333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32c8027ac51489eb749b9c0f46fa914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 train acc 0.9914772727272727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ae003e24da406b81621664282ef9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 test acc 0.8880208333333334\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        # if batch_id % log_interval == 0:\n",
    "        #     print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73f42ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vals, idx):\n",
    "    valscpu = vals.cpu().detach().squeeze(0)\n",
    "    a = 0\n",
    "    for i in valscpu:\n",
    "        a += np.exp(i)\n",
    "    return ((np.exp(valscpu[idx]))/a).item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f22f2f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"news.pt\")\n",
    "modelload = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "modelload.load_state_dict(torch.load(\"news.pt\", device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5481ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStockData(symbol, startDate, endDate):\n",
    "    df_stock = fdr.DataReader(symbol, startDate.isoformat(), endDate.isoformat())\n",
    "    df_stock = df_stock[['Close']]\n",
    "    df_stock['Fluctuation'] = df_stock['Close'].div(df_stock['Close'].shift(1)).apply(lambda x : (x - 1) * 100)\n",
    "    return df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46319cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateTitles(companyName, url):\n",
    "    resp = requests.get(url)\n",
    "    titles = []\n",
    "    cate = [\"정치\",\"경제\",\"사회\", \"생활/문화\",\"세계\",\"기술/IT\", \"연예\", \"스포츠\"]\n",
    "\n",
    "    for item in bs(resp.text, 'xml').find_all('item'):\n",
    "        title = item.title.string\n",
    "        source = item.source.string # 언론사\n",
    "        if(companyName in title):\n",
    "            newtitle = str(title.encode('utf-8'))\n",
    "            newtitle = re.sub(companyName,\"\",title)\n",
    "            tmp = [newtitle]\n",
    "            transform = nlp.data.BERTSentenceTransform(tok, max_len, pad=True, pair=False)\n",
    "            tokenized = transform(tmp)\n",
    "            \n",
    "            modelload.eval()\n",
    "            result = model(torch.tensor([tokenized[0]]).to(device), [tokenized[1]], torch.tensor(tokenized[2]).to(device))\n",
    "            idx = result.argmax().cpu().item()\n",
    "            if(idx==1 or idx==5):\n",
    "                titles.append(title[:title.find(source) - 3])\n",
    "\n",
    "    return ' '.join(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6ec4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyFluctuation(fluctuation):\n",
    "    if fluctuation < -2.5:\n",
    "        return 0\n",
    "    elif fluctuation < 0:\n",
    "        return 1\n",
    "    elif fluctuation < 2.5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bfd0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(companyName, startDate, endDate, isKor=True): \n",
    "    if isKor:\n",
    "        country = ('ko', 'KR')\n",
    "        symbol = str(df_kospi.loc[df_kospi['Name'] == companyName]['Symbol'].values[0])\n",
    "        print(f'Start crawling for {companyName} in Google News Korea')\n",
    "    else:\n",
    "        country = ('en', 'US')\n",
    "        symbol = df_snp.loc[df_snp['Name'] == companyName]['Symbol'].values[0]\n",
    "        print(f'Start crawling for {companyName} in Google News US')\n",
    "\n",
    "    df_stock = loadStockData(symbol, startDate - datetime.timedelta(days=1), endDate)\n",
    "    # df_stock.to_csv(f'./stock/{country[1]}/{companyName}_{startDate.isoformat()}_{endDate.isoformat()}.csv')\n",
    "    print(f'Loaded {companyName} price info, from {startDate.isoformat()} to {endDate.isoformat()}!')\n",
    "\n",
    "    dateList = df_stock.index.map(lambda x: datetime.datetime.strftime(x, '%Y-%m-%d')).values\n",
    "    fluctuationList = df_stock.loc[:, 'Fluctuation'].values\n",
    "\n",
    "    idx = 1\n",
    "    while idx < len(dateList):\n",
    "        url = f'https://news.google.com/rss/search?q={companyName}+after:{dateList[idx - 1]}+before:{dateList[idx]}& \\\n",
    "                hl={country[0]}&gl={country[1]}&ceid={country[1]}:{country[0]}'\n",
    "        aggTitle = aggregateTitles(companyName, url)\n",
    "        if aggTitle:\n",
    "            with open(f'./new_exp_2018/{classifyFluctuation(fluctuationList[idx])}/{companyName}_{dateList[idx]}.txt', \n",
    "                        'w', encoding='UTF-8') as file:\n",
    "                file.write(aggTitle)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a616d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start crawling for 삼성전자 in Google News Korea\n",
      "Loaded 삼성전자 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for SK하이닉스 in Google News Korea\n",
      "Loaded SK하이닉스 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for NAVER in Google News Korea\n",
      "Loaded NAVER price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 삼성바이오로직스 in Google News Korea\n",
      "Loaded 삼성바이오로직스 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 카카오 in Google News Korea\n",
      "Loaded 카카오 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for LG화학 in Google News Korea\n",
      "Loaded LG화학 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 삼성SDI in Google News Korea\n",
      "Loaded 삼성SDI price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 현대차 in Google News Korea\n",
      "Loaded 현대차 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 기아 in Google News Korea\n",
      "Loaded 기아 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 셀트리온 in Google News Korea\n",
      "Loaded 셀트리온 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 카카오뱅크 in Google News Korea\n",
      "Loaded 카카오뱅크 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 크래프톤 in Google News Korea\n",
      "Loaded 크래프톤 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for POSCO in Google News Korea\n",
      "Loaded POSCO price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for KB금융 in Google News Korea\n",
      "Loaded KB금융 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 현대모비스 in Google News Korea\n",
      "Loaded 현대모비스 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 카카오페이 in Google News Korea\n",
      "Loaded 카카오페이 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 삼성물산 in Google News Korea\n",
      "Loaded 삼성물산 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for SK이노베이션 in Google News Korea\n",
      "Loaded SK이노베이션 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for LG전자 in Google News Korea\n",
      "Loaded LG전자 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 신한지주 in Google News Korea\n",
      "Loaded 신한지주 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for LG생활건강 in Google News Korea\n",
      "Loaded LG생활건강 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for SK바이오사이언스 in Google News Korea\n",
      "Loaded SK바이오사이언스 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 하이브 in Google News Korea\n",
      "Loaded 하이브 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 엔씨소프트 in Google News Korea\n",
      "Loaded 엔씨소프트 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 한국전력 in Google News Korea\n",
      "Loaded 한국전력 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 삼성생명 in Google News Korea\n",
      "Loaded 삼성생명 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 두산중공업 in Google News Korea\n",
      "Loaded 두산중공업 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 하나금융지주 in Google News Korea\n",
      "Loaded 하나금융지주 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for HMM in Google News Korea\n",
      "Loaded HMM price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 삼성전기 in Google News Korea\n",
      "Loaded 삼성전기 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 삼성에스디에스 in Google News Korea\n",
      "Loaded 삼성에스디에스 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for SK아이이테크놀로지 in Google News Korea\n",
      "Loaded SK아이이테크놀로지 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for KT&G in Google News Korea\n",
      "Loaded KT&G price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 넷마블 in Google News Korea\n",
      "Loaded 넷마블 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 포스코케미칼 in Google News Korea\n",
      "Loaded 포스코케미칼 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 아모레퍼시픽 in Google News Korea\n",
      "Loaded 아모레퍼시픽 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 삼성화재 in Google News Korea\n",
      "Loaded 삼성화재 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 대한항공 in Google News Korea\n",
      "Loaded 대한항공 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for S-Oil in Google News Korea\n",
      "Loaded S-Oil price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 우리금융지주 in Google News Korea\n",
      "Loaded 우리금융지주 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 현대중공업 in Google News Korea\n",
      "Loaded 현대중공업 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 고려아연 in Google News Korea\n",
      "Loaded 고려아연 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 기업은행 in Google News Korea\n",
      "Loaded 기업은행 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for KT in Google News Korea\n",
      "Loaded KT price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for SK바이오팜 in Google News Korea\n",
      "Loaded SK바이오팜 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for LG디스플레이 in Google News Korea\n",
      "Loaded LG디스플레이 price info, from 2018-01-01 to 2018-12-31!\n",
      "Start crawling for 한온시스템 in Google News Korea\n",
      "Loaded 한온시스템 price info, from 2018-01-01 to 2018-12-31!\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    df_kospi = fdr.StockListing('KOSPI')\n",
    "    df_snp = fdr.StockListing('S&P500')\n",
    "    # May replace w/ fixed dictionary\n",
    "\n",
    "    startDate = datetime.date(2018, 1, 1) # inclusive\n",
    "    endDate = datetime.date(2018, 12, 31) # inclusive\n",
    "    companyListK = ['삼성전자', 'SK하이닉스', 'NAVER', '삼성바이오로직스', '카카오', 'LG화학', '삼성SDI', \n",
    "                    '현대차', '기아', '셀트리온', '카카오뱅크', '크래프톤', 'POSCO', 'KB금융', '현대모비스', \n",
    "                    '카카오페이', '삼성물산', 'SK이노베이션', 'LG전자', '신한지주', 'LG생활건강', 'SK바이오사이언스', \n",
    "                    '하이브', '엔씨소프트', '한국전력', '삼성생명', '두산중공업', '하나금융지주', 'HMM', '삼성전기', \n",
    "                    '삼성에스디에스', 'SK아이이테크놀로지', 'KT&G', '넷마블', '포스코케미칼', '아모레퍼시픽', '삼성화재', \n",
    "                    '대한항공', 'S-Oil', '우리금융지주', '현대중공업', '고려아연', '기업은행', 'KT', 'SK바이오팜', 'LG디스플레이', '한온시스템']\n",
    "    # 우리금융지주 수집 중 \"Remote end closed connection without\" urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
    "    # KOSPI 시총 상위 50개 종목, 지주회사 제외\n",
    "\n",
    "    for companyName in companyListK:\n",
    "        crawl(companyName, startDate, endDate)\n",
    "\n",
    "    # companyListUS = ['Apple', 'IBM', 'Delta Air Lines']\n",
    "    # for companyName in companyListUS:\n",
    "    #     crawl(companyName, startDate, endDate, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a22e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
