{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5edbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c47369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3717bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816e6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b421f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664ccad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2721 802\n"
     ]
    }
   ],
   "source": [
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "root = \"exp_2020/\"\n",
    "list = os.listdir(root)\n",
    "for cat in list:\n",
    "    files = os.listdir(root + cat)\n",
    "    for i,f in enumerate(files):\n",
    "        if (f == '.ipynb_checkpoints'):\n",
    "            continue\n",
    "        fname = root + cat + \"/\" + f\n",
    "        file = open(fname, \"r\", encoding=\"utf-8\")\n",
    "        strings = file.read()\n",
    "        #mynum = len(files) * 0.75\n",
    "        if i<480:\n",
    "            dataset_train.append([strings, cat])\n",
    "        elif i<604:\n",
    "            dataset_test.append([strings,cat])\n",
    "        file.close()\n",
    "        \n",
    "root2 = \"exp_2019/\"\n",
    "list = os.listdir(root2)\n",
    "for cat in list:\n",
    "    files = os.listdir(root2 + cat)\n",
    "    for i,f in enumerate(files):\n",
    "        if (f == '.ipynb_checkpoints'):\n",
    "            continue\n",
    "        fname = root2 + cat + \"/\" + f\n",
    "        file = open(fname, \"r\", encoding=\"utf-8\")\n",
    "        strings = file.read()\n",
    "        #mynum = len(files) * 0.75\n",
    "        if i<201:\n",
    "            dataset_train.append([strings, cat])\n",
    "        elif i<278:\n",
    "            dataset_test.append([strings,cat])\n",
    "        file.close()\n",
    "\n",
    "print(len(dataset_train), len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "187d4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자, 2020년 지속가능경영보고서 발간 [단독 인터뷰] 中 기업 가는 장원기 전 삼성전자 사장 “기술유출? 삼\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[0][0][:64]) #sentence\n",
    "print(dataset_train[0][1]) #label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cf24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx):\n",
    "        # 현재 i[sent_idx] 가 본문\n",
    "        self.sentences = [i[sent_idx][:64] for i in dataset]\n",
    "        self.labels = [i[label_idx] for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd8e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_epochs = 40\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bdeff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1)\n",
    "data_test = BERTDataset(dataset_test, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1759eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "395b9491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b75a85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HMM, 싱가포르항에 전용 터미널 확보 추진', \"현대차 美서 질주…첨단기술 만족도 1위 현대차, 미국 소비자가 뽑은 첨단 기술 만족도 1위 車브랜드 '모델명? 서브\", 'LG디스플레이, TV 줄이고 IT 중심 다각화 눈길 LG디스플레이 파주사업장 직원 코로나19 확진 LG디스플레이 파', \"삼바 vs 셀트리온, 美서 '허셉틴' 바이오시밀러 경쟁 예고\")\n",
      "tensor([0, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for text,label in train_dataloader:\n",
    "    print(text)\n",
    "    result = tuple((int(x[0])) for x in label)\n",
    "    result = torch.tensor(result)\n",
    "    print(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd3bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e7bd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/40] -> Train Loss: 951.0277, Accuracy: 0.264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/40] -> Test Accuracy: 0.248\n",
      "[Epoch 2/40] -> Train Loss: 947.5280, Accuracy: 0.251\n",
      "[Epoch 2/40] -> Test Accuracy: 0.251\n",
      "[Epoch 3/40] -> Train Loss: 946.9815, Accuracy: 0.249\n",
      "[Epoch 3/40] -> Test Accuracy: 0.251\n",
      "[Epoch 4/40] -> Train Loss: 948.1542, Accuracy: 0.258\n",
      "[Epoch 4/40] -> Test Accuracy: 0.246\n",
      "[Epoch 5/40] -> Train Loss: 947.4507, Accuracy: 0.244\n",
      "[Epoch 5/40] -> Test Accuracy: 0.233\n",
      "[Epoch 6/40] -> Train Loss: 947.4057, Accuracy: 0.249\n",
      "[Epoch 6/40] -> Test Accuracy: 0.259\n",
      "[Epoch 7/40] -> Train Loss: 945.6025, Accuracy: 0.263\n",
      "[Epoch 7/40] -> Test Accuracy: 0.231\n",
      "[Epoch 8/40] -> Train Loss: 945.1024, Accuracy: 0.259\n",
      "[Epoch 8/40] -> Test Accuracy: 0.221\n",
      "[Epoch 9/40] -> Train Loss: 941.9778, Accuracy: 0.276\n",
      "[Epoch 9/40] -> Test Accuracy: 0.248\n",
      "[Epoch 10/40] -> Train Loss: 944.1598, Accuracy: 0.252\n",
      "[Epoch 10/40] -> Test Accuracy: 0.221\n",
      "[Epoch 11/40] -> Train Loss: 941.2367, Accuracy: 0.272\n",
      "[Epoch 11/40] -> Test Accuracy: 0.222\n",
      "[Epoch 12/40] -> Train Loss: 937.5149, Accuracy: 0.291\n",
      "[Epoch 12/40] -> Test Accuracy: 0.232\n",
      "[Epoch 13/40] -> Train Loss: 935.6992, Accuracy: 0.286\n",
      "[Epoch 13/40] -> Test Accuracy: 0.201\n",
      "[Epoch 14/40] -> Train Loss: 935.5689, Accuracy: 0.290\n",
      "[Epoch 14/40] -> Test Accuracy: 0.203\n",
      "[Epoch 15/40] -> Train Loss: 924.0449, Accuracy: 0.310\n",
      "[Epoch 15/40] -> Test Accuracy: 0.186\n",
      "[Epoch 16/40] -> Train Loss: 916.9548, Accuracy: 0.318\n",
      "[Epoch 16/40] -> Test Accuracy: 0.203\n",
      "[Epoch 17/40] -> Train Loss: 901.1388, Accuracy: 0.349\n",
      "[Epoch 17/40] -> Test Accuracy: 0.208\n",
      "[Epoch 18/40] -> Train Loss: 893.5485, Accuracy: 0.351\n",
      "[Epoch 18/40] -> Test Accuracy: 0.217\n",
      "[Epoch 19/40] -> Train Loss: 878.1226, Accuracy: 0.387\n",
      "[Epoch 19/40] -> Test Accuracy: 0.222\n",
      "[Epoch 20/40] -> Train Loss: 875.0240, Accuracy: 0.376\n",
      "[Epoch 20/40] -> Test Accuracy: 0.218\n",
      "[Epoch 21/40] -> Train Loss: 862.2858, Accuracy: 0.391\n",
      "[Epoch 21/40] -> Test Accuracy: 0.222\n",
      "[Epoch 22/40] -> Train Loss: 838.4759, Accuracy: 0.402\n",
      "[Epoch 22/40] -> Test Accuracy: 0.214\n",
      "[Epoch 23/40] -> Train Loss: 833.7485, Accuracy: 0.419\n",
      "[Epoch 23/40] -> Test Accuracy: 0.206\n",
      "[Epoch 24/40] -> Train Loss: 823.3457, Accuracy: 0.420\n",
      "[Epoch 24/40] -> Test Accuracy: 0.226\n",
      "[Epoch 25/40] -> Train Loss: 802.8271, Accuracy: 0.426\n",
      "[Epoch 25/40] -> Test Accuracy: 0.241\n",
      "[Epoch 26/40] -> Train Loss: 791.1589, Accuracy: 0.448\n",
      "[Epoch 26/40] -> Test Accuracy: 0.237\n",
      "[Epoch 27/40] -> Train Loss: 782.2815, Accuracy: 0.449\n",
      "[Epoch 27/40] -> Test Accuracy: 0.211\n",
      "[Epoch 28/40] -> Train Loss: 764.9925, Accuracy: 0.470\n",
      "[Epoch 28/40] -> Test Accuracy: 0.207\n",
      "[Epoch 29/40] -> Train Loss: 749.5497, Accuracy: 0.484\n",
      "[Epoch 29/40] -> Test Accuracy: 0.224\n",
      "[Epoch 30/40] -> Train Loss: 739.7854, Accuracy: 0.493\n",
      "[Epoch 30/40] -> Test Accuracy: 0.229\n",
      "[Epoch 31/40] -> Train Loss: 717.1997, Accuracy: 0.509\n",
      "[Epoch 31/40] -> Test Accuracy: 0.203\n",
      "[Epoch 32/40] -> Train Loss: 707.9231, Accuracy: 0.515\n",
      "[Epoch 32/40] -> Test Accuracy: 0.217\n",
      "[Epoch 33/40] -> Train Loss: 683.1231, Accuracy: 0.529\n",
      "[Epoch 33/40] -> Test Accuracy: 0.227\n",
      "[Epoch 34/40] -> Train Loss: 668.8557, Accuracy: 0.541\n",
      "[Epoch 34/40] -> Test Accuracy: 0.216\n",
      "[Epoch 35/40] -> Train Loss: 661.0464, Accuracy: 0.552\n",
      "[Epoch 35/40] -> Test Accuracy: 0.229\n",
      "[Epoch 36/40] -> Train Loss: 652.0907, Accuracy: 0.545\n",
      "[Epoch 36/40] -> Test Accuracy: 0.222\n",
      "[Epoch 37/40] -> Train Loss: 622.4949, Accuracy: 0.581\n",
      "[Epoch 37/40] -> Test Accuracy: 0.218\n",
      "[Epoch 38/40] -> Train Loss: 620.5452, Accuracy: 0.584\n",
      "[Epoch 38/40] -> Test Accuracy: 0.223\n",
      "[Epoch 39/40] -> Train Loss: 598.0361, Accuracy: 0.606\n",
      "[Epoch 39/40] -> Test Accuracy: 0.211\n",
      "[Epoch 40/40] -> Train Loss: 586.3329, Accuracy: 0.598\n",
      "[Epoch 40/40] -> Test Accuracy: 0.217\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n",
    "itr=1\n",
    "p_itr=200\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    for sentence, label in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in sentence]\n",
    "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "        \n",
    "        sample = torch.tensor(padded_list)\n",
    "        label = tuple((int(x[0])) for x in label)\n",
    "        label = torch.tensor(label)\n",
    "        sample = sample.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        labels = torch.tensor(label)\n",
    "        outputs = model(sample, labels=labels)\n",
    "        loss, logits = outputs\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)        \n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        #scheduler.step()    \n",
    "    print('[Epoch {}/{}] -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, num_epochs, total_loss, total_correct/total_len))\n",
    "    \n",
    "    model.eval()\n",
    "    t_total_correct = 0\n",
    "    t_total_len = 0\n",
    "    for sentence, label in test_dataloader:\n",
    "        encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in sentence]\n",
    "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "        \n",
    "        sample = torch.tensor(padded_list)\n",
    "        label = tuple((int(x[0])) for x in label)\n",
    "        label = torch.tensor(label)\n",
    "        sample = sample.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        labels = torch.tensor(label)\n",
    "        outputs = model(sample, labels=labels)\n",
    "        _, logits = outputs\n",
    "        \n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)        \n",
    "        correct = pred.eq(labels)\n",
    "        t_total_correct += correct.sum().item()\n",
    "        t_total_len += len(labels)\n",
    "    print('[Epoch {}/{}] -> Test Accuracy: {:.3f}'.format(epoch+1, num_epochs, t_total_correct/t_total_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e8ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n",
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.28233830845771146\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "\n",
    "for sentence, label in test_dataloader:\n",
    "    encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in sentence]\n",
    "    padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "    sample = torch.tensor(padded_list)\n",
    "    label = tuple((int(x[0])) for x in label)\n",
    "    label = torch.tensor(label)\n",
    "    sample = sample.to(device)\n",
    "    label = label.to(device)\n",
    "        \n",
    "    labels = torch.tensor(label)\n",
    "    outputs = model(sample, labels=labels)\n",
    "    _, logits = outputs\n",
    "\n",
    "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "    correct = pred.eq(labels)\n",
    "    total_correct += correct.sum().item()\n",
    "    total_len += len(labels)\n",
    "\n",
    "print('Test accuracy: ', total_correct / total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d4df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vals, idx):\n",
    "    valscpu = vals.cpu().detach().squeeze(0)\n",
    "    a = 0\n",
    "    for i in valscpu:\n",
    "        a += np.exp(i)\n",
    "    return ((np.exp(valscpu[idx]))/a).item() * 100\n",
    "\n",
    "def testModel(model, seq):\n",
    "    cate = [\"대폭 하락\",\"소폭 하락\",\"소폭 상승\", \"대폭 상승\"]\n",
    "    tmp = [seq]\n",
    "    encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in tmp]\n",
    "    padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "    sample = torch.tensor(padded_list)\n",
    "\n",
    "    labels = torch.tensor([1]).unsqueeze(0)\n",
    "    sample = sample.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(sample, labels=labels)\n",
    "    _, logits = outputs\n",
    "\n",
    "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "\n",
    "    print(\"주가는:\", cate[pred])\n",
    "    print(\"신뢰도는:\", \"{:.2f}%\".format(softmax(logits,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f98b5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주가는: 대폭 상승\n",
      "신뢰도는: 88.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "testModel(model, \"삼성전자 실적 폭락 공장 불나고 난리남\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b214e520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주가는: 대폭 상승\n",
      "신뢰도는: 95.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "testModel(model, \"대박 갤럭시 최고 증가 1등\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c820b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
