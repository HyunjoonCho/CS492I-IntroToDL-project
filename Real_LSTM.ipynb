{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2f0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 16:36:08.301556: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-03 16:36:08.301594: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Lambda\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70eb6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d7b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99e5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyModel(companyName, predictDate):\n",
    "    symbol = str(df_kospi.loc[df_kospi['Name'] == companyName]['Symbol'].values[0])\n",
    "    companytable = fdr.DataReader(symbol)\n",
    "    companytable['Year'] = companytable.index.year\n",
    "    companytable['Month'] = companytable.index.month\n",
    "    companytable['Day'] = companytable.index.day\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scale_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    scaled = scaler.fit_transform(companytable[scale_cols])\n",
    "    df = pd.DataFrame(scaled, columns=scale_cols)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('Close', 1), df['Close'], test_size=0.4, random_state=0, shuffle=False)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=0, shuffle=False)\n",
    "    \n",
    "    WINDOW_SIZE=20\n",
    "    BATCH_SIZE=32\n",
    "    \n",
    "    train_data = windowed_dataset(y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "    validate_data = windowed_dataset(y_val, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "    test_data = windowed_dataset(y_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # 1차원 feature map 생성\n",
    "        Conv1D(filters=32, kernel_size=5,\n",
    "               padding=\"causal\",\n",
    "               activation=\"relu\",\n",
    "               input_shape=[WINDOW_SIZE, 1]),\n",
    "        # LSTM\n",
    "        LSTM(16, activation='tanh'),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1),\n",
    "    ])\n",
    "    \n",
    "    loss = Huber()\n",
    "    optimizer = Adam(0.0005)\n",
    "    model.compile(loss=Huber(), optimizer=optimizer, metrics=['mse'])\n",
    "    \n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    filename = os.path.join('tmp', 'ckeckpointer.ckpt')\n",
    "    checkpoint = ModelCheckpoint(filename, \n",
    "                                 save_weights_only=True, \n",
    "                                 save_best_only=True, \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1)\n",
    "    \n",
    "    history = model.fit(train_data, \n",
    "                        validation_data=(validate_data), \n",
    "                        epochs=50, \n",
    "                        callbacks=[checkpoint, earlystopping])\n",
    "    \n",
    "    model.load_weights(filename)\n",
    "    \n",
    "    #주어진 데이터 적용\n",
    "    forpredict = fdr.DataReader(symbol, predictDate - datetime.timedelta(days=40), predictDate)\n",
    "    forpredict = forpredict[-20:]\n",
    "    scaler = MinMaxScaler()\n",
    "    scale_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    scaled = scaler.fit_transform(forpredict[scale_cols])\n",
    "    df = pd.DataFrame(scaled, columns=scale_cols)\n",
    "    my_test = df['Close']\n",
    "    my_test.loc['20'] = 0\n",
    "    my_test_data = windowed_dataset(my_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "    my_pred = model.predict(my_test_data)\n",
    "    \n",
    "    myzeros = np.zeros((my_pred.shape[0],5))\n",
    "    newpred = my_pred + myzeros\n",
    "    price_pred = scaler.inverse_transform(newpred)\n",
    "    price_pred = price_pred[:,3:4]\n",
    "    \n",
    "    return price_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551e430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 16:36:21.553019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-03 16:36:21.553486: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-03 16:36:21.553553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-03 16:36:21.553615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-03 16:36:21.555450: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-03 16:36:21.555516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-12-03 16:36:21.555636: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-03 16:36:21.556101: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "    111/Unknown - 2s 5ms/step - loss: 2.7953e-04 - mse: 5.5905e-04\n",
      "Epoch 00001: val_loss improved from inf to 0.00303, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 2s 11ms/step - loss: 2.7764e-04 - mse: 5.5529e-04 - val_loss: 0.0030 - val_mse: 0.0061\n",
      "Epoch 2/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.6746e-05 - mse: 5.3492e-05\n",
      "Epoch 00002: val_loss did not improve from 0.00303\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.8070e-05 - mse: 5.6140e-05 - val_loss: 0.0048 - val_mse: 0.0095\n",
      "Epoch 3/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.5899e-05 - mse: 5.1797e-05\n",
      "Epoch 00003: val_loss did not improve from 0.00303\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.7268e-05 - mse: 5.4536e-05 - val_loss: 0.0042 - val_mse: 0.0083\n",
      "Epoch 4/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.4285e-05 - mse: 4.8569e-05\n",
      "Epoch 00004: val_loss improved from 0.00303 to 0.00288, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.5996e-05 - mse: 5.1992e-05 - val_loss: 0.0029 - val_mse: 0.0058\n",
      "Epoch 5/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.3907e-05 - mse: 4.7815e-05\n",
      "Epoch 00005: val_loss did not improve from 0.00288\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.4959e-05 - mse: 4.9918e-05 - val_loss: 0.0030 - val_mse: 0.0061\n",
      "Epoch 6/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.1806e-05 - mse: 4.3611e-05\n",
      "Epoch 00006: val_loss did not improve from 0.00288\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.4525e-05 - mse: 4.9050e-05 - val_loss: 0.0035 - val_mse: 0.0069\n",
      "Epoch 7/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.2661e-05 - mse: 4.5321e-05\n",
      "Epoch 00007: val_loss did not improve from 0.00288\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.3732e-05 - mse: 4.7463e-05 - val_loss: 0.0031 - val_mse: 0.0062\n",
      "Epoch 8/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.1798e-05 - mse: 4.3596e-05\n",
      "Epoch 00008: val_loss improved from 0.00288 to 0.00235, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.3319e-05 - mse: 4.6638e-05 - val_loss: 0.0023 - val_mse: 0.0047\n",
      "Epoch 9/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.1895e-05 - mse: 4.3790e-05\n",
      "Epoch 00009: val_loss improved from 0.00235 to 0.00218, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.3034e-05 - mse: 4.6069e-05 - val_loss: 0.0022 - val_mse: 0.0044\n",
      "Epoch 10/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.3719e-05 - mse: 4.7437e-05\n",
      "Epoch 00010: val_loss did not improve from 0.00218\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.4791e-05 - mse: 4.9583e-05 - val_loss: 0.0024 - val_mse: 0.0047\n",
      "Epoch 11/50\n",
      "101/112 [==========================>...] - ETA: 0s - loss: 1.9786e-05 - mse: 3.9573e-05\n",
      "Epoch 00011: val_loss improved from 0.00218 to 0.00185, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.1940e-05 - mse: 4.3881e-05 - val_loss: 0.0019 - val_mse: 0.0037\n",
      "Epoch 12/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.1310e-05 - mse: 4.2620e-05\n",
      "Epoch 00012: val_loss improved from 0.00185 to 0.00184, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.2130e-05 - mse: 4.4260e-05 - val_loss: 0.0018 - val_mse: 0.0037\n",
      "Epoch 13/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.0440e-05 - mse: 4.0880e-05\n",
      "Epoch 00013: val_loss improved from 0.00184 to 0.00179, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.1215e-05 - mse: 4.2429e-05 - val_loss: 0.0018 - val_mse: 0.0036\n",
      "Epoch 14/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.8549e-05 - mse: 3.7099e-05\n",
      "Epoch 00014: val_loss improved from 0.00179 to 0.00157, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.9661e-05 - mse: 3.9321e-05 - val_loss: 0.0016 - val_mse: 0.0031\n",
      "Epoch 15/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.8775e-05 - mse: 3.7550e-05\n",
      "Epoch 00015: val_loss improved from 0.00157 to 0.00156, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.9590e-05 - mse: 3.9180e-05 - val_loss: 0.0016 - val_mse: 0.0031\n",
      "Epoch 16/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.8935e-05 - mse: 3.7870e-05\n",
      "Epoch 00016: val_loss did not improve from 0.00156\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.0245e-05 - mse: 4.0491e-05 - val_loss: 0.0016 - val_mse: 0.0031\n",
      "Epoch 17/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 2.0723e-05 - mse: 4.1445e-05\n",
      "Epoch 00017: val_loss did not improve from 0.00156\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 2.1409e-05 - mse: 4.2818e-05 - val_loss: 0.0022 - val_mse: 0.0043\n",
      "Epoch 18/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.7934e-05 - mse: 3.5869e-05\n",
      "Epoch 00018: val_loss did not improve from 0.00156\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.8919e-05 - mse: 3.7838e-05 - val_loss: 0.0020 - val_mse: 0.0039\n",
      "Epoch 19/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.6338e-05 - mse: 3.2675e-05\n",
      "Epoch 00019: val_loss improved from 0.00156 to 0.00123, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.7778e-05 - mse: 3.5556e-05 - val_loss: 0.0012 - val_mse: 0.0025\n",
      "Epoch 20/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.6579e-05 - mse: 3.3159e-05\n",
      "Epoch 00020: val_loss improved from 0.00123 to 0.00123, saving model to tmp/ckeckpointer.ckpt\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.7132e-05 - mse: 3.4264e-05 - val_loss: 0.0012 - val_mse: 0.0025\n",
      "Epoch 21/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.5700e-05 - mse: 3.1400e-05\n",
      "Epoch 00021: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.6882e-05 - mse: 3.3764e-05 - val_loss: 0.0013 - val_mse: 0.0027\n",
      "Epoch 22/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.7196e-05 - mse: 3.4392e-05\n",
      "Epoch 00022: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.8139e-05 - mse: 3.6277e-05 - val_loss: 0.0021 - val_mse: 0.0042\n",
      "Epoch 23/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.6364e-05 - mse: 3.2728e-05\n",
      "Epoch 00023: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.6836e-05 - mse: 3.3673e-05 - val_loss: 0.0017 - val_mse: 0.0034\n",
      "Epoch 24/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.4721e-05 - mse: 2.9442e-05\n",
      "Epoch 00024: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.5982e-05 - mse: 3.1964e-05 - val_loss: 0.0017 - val_mse: 0.0034\n",
      "Epoch 25/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.5751e-05 - mse: 3.1502e-05\n",
      "Epoch 00025: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.6306e-05 - mse: 3.2611e-05 - val_loss: 0.0019 - val_mse: 0.0038\n",
      "Epoch 26/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.5243e-05 - mse: 3.0485e-05\n",
      "Epoch 00026: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.6130e-05 - mse: 3.2259e-05 - val_loss: 0.0016 - val_mse: 0.0032\n",
      "Epoch 27/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.5040e-05 - mse: 3.0080e-05\n",
      "Epoch 00027: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.5624e-05 - mse: 3.1248e-05 - val_loss: 0.0017 - val_mse: 0.0034\n",
      "Epoch 28/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.4450e-05 - mse: 2.8901e-05\n",
      "Epoch 00028: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.5285e-05 - mse: 3.0570e-05 - val_loss: 0.0023 - val_mse: 0.0046\n",
      "Epoch 29/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.4069e-05 - mse: 2.8139e-05\n",
      "Epoch 00029: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.5107e-05 - mse: 3.0214e-05 - val_loss: 0.0024 - val_mse: 0.0048\n",
      "Epoch 30/50\n",
      "102/112 [==========================>...] - ETA: 0s - loss: 1.3565e-05 - mse: 2.7129e-05\n",
      "Epoch 00030: val_loss did not improve from 0.00123\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.4215e-05 - mse: 2.8430e-05 - val_loss: 0.0022 - val_mse: 0.0044\n"
     ]
    }
   ],
   "source": [
    "df_kospi = fdr.StockListing('KOSPI')\n",
    "\n",
    "predict_Date = datetime.date(2021 , 12, 4) \n",
    "#주가 예측을 원하는 날짜\n",
    "#현재는 하루 이후만 가능함, 과거 날짜는 모두 괜찮음(아직 안나온 기준 하루)\n",
    "predict_Company = '호텔신라' \n",
    "#주가 예측을 원하는 기업\n",
    "\n",
    "predicted_price = applyModel(predict_Company, predict_Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "412734e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 가격은 73902.14283168316 원\n"
     ]
    }
   ],
   "source": [
    "print(\"예측 가격은\", predicted_price[0][0],\"원\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38618f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
