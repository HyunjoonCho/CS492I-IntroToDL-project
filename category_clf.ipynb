{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6e7ae9bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e7ae9bc",
        "outputId": "26819efb-183b-4e41-fe36-677f501d1afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.10.0+cu111)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 37.7 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.10.0.2)\n",
            "Collecting botocore<1.24.0,>=1.23.23\n",
            "  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 33.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3->pytorch-transformers) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.23 botocore-1.23.23 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 urllib3-1.25.11\n",
            "Cloning into 'CS492I-IntroToDL-project'...\n",
            "remote: Enumerating objects: 23179, done.\u001b[K\n",
            "remote: Counting objects: 100% (23179/23179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22685/22685), done.\u001b[K\n",
            "remote: Total 23179 (delta 455), reused 22961 (delta 261), pack-reused 0\n",
            "Receiving objects: 100% (23179/23179), 17.77 MiB | 19.48 MiB/s, done.\n",
            "Resolving deltas: 100% (455/455), done.\n",
            "Checking out files: 100% (22189/22189), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers\n",
        "!git clone https://github.com/HyunjoonCho/CS492I-IntroToDL-project.git\n",
        "import os\n",
        "os.chdir('CS492I-IntroToDL-project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9f5e34f4",
      "metadata": {
        "id": "9f5e34f4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "66beebd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66beebd0",
        "outputId": "ed72c985-47c5-4101-ac33-374baeb9965d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "['BertForSequenceClassification_Category_best.ckpt', 'BertForSequenceClassification_Fluctuation_best.ckpt']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "drive_root = '/gdrive/My Drive/CS492I/project-pretrain'\n",
        "print(os.listdir(Path(drive_root)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a465b83f",
      "metadata": {
        "id": "a465b83f"
      },
      "outputs": [],
      "source": [
        "from easydict import EasyDict as edict\n",
        "\n",
        "args = edict()\n",
        "args.gpu = True\n",
        "args.batch_size = 4\n",
        "args.num_epochs = 15\n",
        "args.learning_rate = 1e-6\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eeb76c36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeb76c36",
        "outputId": "9b8d7f4a-1a82-488b-addc-eff945e1859c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 995526/995526 [00:00<00:00, 5737214.10B/s]\n",
            "100%|██████████| 625/625 [00:00<00:00, 206868.69B/s]\n",
            "100%|██████████| 714314041/714314041 [00:21<00:00, 32774785.02B/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "bert_category_clf = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "da9d64f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da9d64f9",
        "outputId": "d3ddebab-4d03-4989-fb55-6c81e1fdfe77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1200 200 200\n"
          ]
        }
      ],
      "source": [
        "dataset_train = []\n",
        "dataset_val = []\n",
        "dataset_test = []\n",
        "\n",
        "root = Path('dataset/category')\n",
        "list = os.listdir(root)\n",
        "for cat in list:\n",
        "    files = os.listdir(root / cat)\n",
        "    for i,f in enumerate(files):\n",
        "        fname = root / cat / f\n",
        "        with open(fname, \"r\", encoding=\"utf-8\") as file:\n",
        "            strings = file.read()\n",
        "            if i < 150:\n",
        "                dataset_train.append([strings, cat])\n",
        "            elif i < 175:\n",
        "                dataset_val.append([strings, cat])\n",
        "            else:\n",
        "                dataset_test.append([strings,cat])\n",
        "\n",
        "print(len(dataset_train), len(dataset_val), len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "52d14c2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52d14c2b",
        "outputId": "c347230f-f683-4a51-a666-b5c546478e81",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터만 사용하는 알뜰폰 나온다\t음성·문자없는 ‘데이터온리’ 상품\n",
            "6월 알뜰폰 활성화 방안에 포함\n",
            "가격경쟁력 대신 \n",
            "5\n"
          ]
        }
      ],
      "source": [
        "print(dataset_train[0][0][:64]) #sentence\n",
        "print(dataset_train[0][1]) #label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3d09442e",
      "metadata": {
        "id": "3d09442e"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx):\n",
        "        # 현재 i[sent_idx] 가 본문\n",
        "        self.sentences = [i[sent_idx][:64] for i in dataset]\n",
        "        self.labels = [i[label_idx] for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.sentences[i], self.labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1ecd671b",
      "metadata": {
        "id": "1ecd671b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1)\n",
        "data_val = BERTDataset(dataset_val, 0, 1)\n",
        "data_test = BERTDataset(dataset_test, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b677a497",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b677a497",
        "outputId": "853e3b0b-0639-4f38-d5de-fc67ad3f372e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(data_train, batch_size=args.batch_size, num_workers=5, shuffle=True)\n",
        "val_dataloader = DataLoader(data_val, batch_size=args.batch_size, num_workers=5, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size=args.batch_size, num_workers=5, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cc73e400",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc73e400",
        "outputId": "42e7c628-44c2-4ff1-ced1-73b56acd43a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "bert_category_clf.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "510a6ea4",
      "metadata": {
        "id": "510a6ea4"
      },
      "outputs": [],
      "source": [
        "def save_model(model, mode='last'):\n",
        "    torch.save(model.state_dict(),  Path('pretrained_models') / f'{type(model).__name__}_Category_{mode}.ckpt')\n",
        "    torch.save(model.state_dict(), Path(drive_root) / f'{type(model).__name__}_Category_{mode}.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d11f854c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11f854c",
        "outputId": "193b1667-d1b5-42cf-8f2d-96efb6bd4b27",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/15] -> Train Loss: 626.4617, Accuracy: 0.126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/15] -> Validation Loss: 103.6096, Accuracy: 0.135\n",
            "[Epoch 2/15] -> Train Loss: 615.9930, Accuracy: 0.186\n",
            "[Epoch 2/15] -> Validation Loss: 102.0773, Accuracy: 0.155\n",
            "[Epoch 3/15] -> Train Loss: 583.0256, Accuracy: 0.294\n",
            "[Epoch 3/15] -> Validation Loss: 91.3869, Accuracy: 0.475\n",
            "[Epoch 4/15] -> Train Loss: 520.7862, Accuracy: 0.468\n",
            "[Epoch 4/15] -> Validation Loss: 79.6258, Accuracy: 0.570\n",
            "[Epoch 5/15] -> Train Loss: 442.3048, Accuracy: 0.573\n",
            "[Epoch 5/15] -> Validation Loss: 69.2369, Accuracy: 0.585\n",
            "[Epoch 6/15] -> Train Loss: 380.9533, Accuracy: 0.658\n",
            "[Epoch 6/15] -> Validation Loss: 65.4018, Accuracy: 0.620\n",
            "[Epoch 7/15] -> Train Loss: 333.5658, Accuracy: 0.699\n",
            "[Epoch 7/15] -> Validation Loss: 57.7941, Accuracy: 0.670\n",
            "[Epoch 8/15] -> Train Loss: 298.3588, Accuracy: 0.742\n",
            "[Epoch 8/15] -> Validation Loss: 54.5529, Accuracy: 0.675\n",
            "[Epoch 9/15] -> Train Loss: 270.4927, Accuracy: 0.770\n",
            "[Epoch 9/15] -> Validation Loss: 50.2049, Accuracy: 0.675\n",
            "[Epoch 10/15] -> Train Loss: 243.3529, Accuracy: 0.791\n",
            "[Epoch 10/15] -> Validation Loss: 49.3664, Accuracy: 0.685\n",
            "[Epoch 11/15] -> Train Loss: 217.3263, Accuracy: 0.814\n",
            "[Epoch 11/15] -> Validation Loss: 47.6338, Accuracy: 0.695\n",
            "[Epoch 12/15] -> Train Loss: 197.1050, Accuracy: 0.838\n",
            "[Epoch 12/15] -> Validation Loss: 45.9465, Accuracy: 0.710\n",
            "[Epoch 13/15] -> Train Loss: 173.1267, Accuracy: 0.860\n",
            "[Epoch 13/15] -> Validation Loss: 44.9932, Accuracy: 0.730\n",
            "[Epoch 14/15] -> Train Loss: 161.4996, Accuracy: 0.863\n",
            "[Epoch 14/15] -> Validation Loss: 43.9865, Accuracy: 0.725\n",
            "[Epoch 15/15] -> Train Loss: 144.7290, Accuracy: 0.880\n",
            "[Epoch 15/15] -> Validation Loss: 41.9845, Accuracy: 0.730\n"
          ]
        }
      ],
      "source": [
        "#training step\n",
        "optimizer = optim.AdamW(bert_category_clf.parameters(), lr=1e-6)\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(args.num_epochs):\n",
        "    train_loss = 0\n",
        "    total_len = 0\n",
        "    total_correct = 0\n",
        "    bert_category_clf.train()\n",
        "    for sentence, label in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in sentence]\n",
        "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "        \n",
        "        sample = torch.tensor(padded_list)\n",
        "        label = tuple((int(x[0])) for x in label)\n",
        "        label = torch.tensor(label)\n",
        "        sample = sample.to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        labels = torch.tensor(label)\n",
        "        loss, logits = bert_category_clf(sample, labels=labels)\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=1)        \n",
        "        correct = pred.eq(labels)\n",
        "        total_correct += correct.sum().item()\n",
        "        total_len += len(labels)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        #scheduler.step()        \n",
        "    print('[Epoch {}/{}] -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, args.num_epochs, train_loss, total_correct/total_len))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bert_category_clf.eval()\n",
        "        val_loss = 0\n",
        "        v_total_correct = 0\n",
        "        v_total_len = 0\n",
        "        for sentence, label in val_dataloader:\n",
        "            encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in sentence]\n",
        "            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "            \n",
        "            sample = torch.tensor(padded_list)\n",
        "            label = tuple((int(x[0])) for x in label)\n",
        "            label = torch.tensor(label)\n",
        "            sample = sample.to(device)\n",
        "            label = label.to(device)\n",
        "            \n",
        "            labels = torch.tensor(label)\n",
        "            loss, logits = bert_category_clf(sample, labels=labels)\n",
        "            \n",
        "            pred = torch.argmax(F.softmax(logits), dim=1)        \n",
        "            correct = pred.eq(labels)\n",
        "            val_loss += loss.item()\n",
        "            v_total_correct += correct.sum().item()\n",
        "            v_total_len += len(labels)\n",
        "        print('[Epoch {}/{}] -> Validation Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, args.num_epochs, val_loss, v_total_correct / v_total_len))\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_model(bert_category_clf, 'best')\n",
        "    save_model(bert_category_clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "93eb1c59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93eb1c59",
        "outputId": "f184a796-1390-4519-9a47-02c33aff1bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.755\n"
          ]
        }
      ],
      "source": [
        "bert_category_clf.eval()\n",
        "\n",
        "t_total_len = 0\n",
        "t_total_correct = 0\n",
        "with torch.no_grad():\n",
        "    for sentence, label in test_dataloader:\n",
        "        encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in sentence]\n",
        "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "        sample = torch.tensor(padded_list)\n",
        "        label = tuple((int(x[0])) for x in label)\n",
        "        label = torch.tensor(label)\n",
        "        sample = sample.to(device)\n",
        "        label = label.to(device)\n",
        "            \n",
        "        labels = torch.tensor(label)\n",
        "        _, logits = bert_category_clf(sample, labels=labels)\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "        correct = pred.eq(labels)\n",
        "        t_total_correct += correct.sum().item()\n",
        "        t_total_len += len(labels)\n",
        "\n",
        "print('Test accuracy: ', t_total_correct / t_total_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9e1ef3d0",
      "metadata": {
        "id": "9e1ef3d0"
      },
      "outputs": [],
      "source": [
        "def test_model(model, seq):\n",
        "    cate = [\"정치\",\"경제\",\"사회\", \"생활/문화\",\"세계\",\"기술/IT\", \"연예\", \"스포츠\"]\n",
        "    tmp = [seq]\n",
        "    encoded_list = [tokenizer.encode(t,add_special_tokens=True) for t in tmp]\n",
        "    padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "\n",
        "    labels = torch.tensor([1]).unsqueeze(0)\n",
        "    sample = sample.to(device)\n",
        "    labels = labels.to(device)\n",
        "    _, logits = model(sample, labels=labels)\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "\n",
        "    print(\"뉴스의 카테고리는:\", cate[pred])\n",
        "    print(\"신뢰도는:\", \"{:.2f}%\".format(F.softmax(logits).max().item() * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ad49b909",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad49b909",
        "outputId": "ef384639-236d-4f22-b159-978ce3ef195d",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "뉴스의 카테고리는: 경제\n",
            "신뢰도는: 53.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ]
        }
      ],
      "source": [
        "test_model(bert_category_clf, \"'승진 끝판왕' 삼성전자 대표이사급 연봉 어느 정도길래?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_category_clf2 = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=8)\n",
        "bert_category_clf2.load_state_dict(torch.load(Path(drive_root) / f'{type(bert_category_clf2).__name__}_Category_best.ckpt', map_location=device))"
      ],
      "metadata": {
        "id": "lUGMObLkNBoY",
        "outputId": "3c4f5220-72a1-4e17-9901-a86ca9fbaa3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lUGMObLkNBoY",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_category_clf2.to(device)\n",
        "test_model(bert_category_clf2, \"'승진 끝판왕' 삼성전자 대표이사급 연봉 어느 정도길래?\")"
      ],
      "metadata": {
        "id": "q7YwsSy3NiZh",
        "outputId": "cff14888-fe0b-4d8e-df27-cfa3f35f54ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q7YwsSy3NiZh",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "뉴스의 카테고리는: 경제\n",
            "신뢰도는: 53.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "category_clf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}